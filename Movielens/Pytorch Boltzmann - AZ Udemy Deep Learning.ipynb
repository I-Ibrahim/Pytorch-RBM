{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data set\n",
    "movies=pd.read_csv('DataMovie/ml-1m/movies.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "users=pd.read_csv('DataMovie/ml-1m/users.dat', sep='::', header=None, engine='python', encoding='latin-1')\n",
    "ratings=pd.read_csv('DataMovie/ml-1m/ratings.dat', sep='::', header=None, engine='python', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training and test set\n",
    "\n",
    "trainingset=pd.read_csv('DataMovie/ml-100k/u1.base', delimiter='\\t', header=None)\n",
    "testset=pd.read_csv('DataMovie/ml-100k/u1.test', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to array as it is quicker \n",
    "\n",
    "trainingset=np.array(trainingset, dtype='int')\n",
    "testset=np.array(testset, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682\n"
     ]
    }
   ],
   "source": [
    "# get total no. of movies and users in order to then make a matrix of the data\n",
    "\n",
    "\n",
    "nb_users= int(max(max(trainingset[:,0]), max(testset[:,0])))\n",
    "nb_movies=int(max(max(trainingset[:,1]), max(testset[:,1])))\n",
    "print(nb_users, nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make matrix of users in lines and movies in columns\n",
    "\n",
    "def convert(data):\n",
    "    new_data=[] #initialise list\n",
    "    for id_users in range(1, nb_users+1):\n",
    "        id_movies=data[:,1][data[:,0]==id_users]\n",
    "        id_ratings=data[:,2][data[:,0]==id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingset=convert(trainingset)\n",
    "testset=convert(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data into torch sensors\n",
    "\n",
    "training_set = torch.FloatTensor(trainingset)\n",
    "test_set=torch.FloatTensor(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     1     1  ...     -1    -1    -1\n",
       "    1    -1    -1  ...     -1    -1    -1\n",
       "   -1    -1    -1  ...     -1    -1    -1\n",
       "       ...          ⋱          ...       \n",
       "    1    -1    -1  ...     -1    -1    -1\n",
       "   -1    -1    -1  ...     -1    -1    -1\n",
       "   -1     1    -1  ...     -1    -1    -1\n",
       "[torch.FloatTensor of size 943x1682]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert ratings (1-5) into binary ratings 1 (liked) and 0 (not liked)\n",
    "\n",
    "training_set[training_set==0] = -1 # not rated\n",
    "training_set[training_set==1] = 0\n",
    "training_set[training_set==2] = 0\n",
    "training_set[training_set>=3] = 1\n",
    "\n",
    "test_set[test_set==0] = -1 # not rated\n",
    "test_set[test_set==1] = 0\n",
    "test_set[test_set==2] = 0\n",
    "test_set[test_set>=3] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_vis=1682,\n",
    "                 n_hin=600,\n",
    "                 k=5):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hin,n_vis)*1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "        self.k = k\n",
    "    \n",
    "    def sample_from_p(self,p):\n",
    "        return F.relu(torch.sign(p - Variable(torch.rand(p.size()))))\n",
    "    \n",
    "    def v_to_h(self,v):\n",
    "        p_h = F.sigmoid(F.linear(v,self.W,self.h_bias))\n",
    "        sample_h = self.sample_from_p(p_h)\n",
    "        return p_h,sample_h\n",
    "    \n",
    "    def h_to_v(self,h):\n",
    "        p_v = F.sigmoid(F.linear(h,self.W.t(),self.v_bias))\n",
    "        sample_v = self.sample_from_p(p_v)\n",
    "        return p_v,sample_v\n",
    "    \n",
    "   # def forward(self,v):\n",
    "    #    pre_h1,h1 = self.v_to_h(v)\n",
    "        \n",
    "     #   h_ = h1\n",
    "      #  for _ in range(self.k):\n",
    "       #     pre_v_,v_ = self.h_to_v(h_)\n",
    "        #    pre_h_,h_ = self.v_to_h(v_)\n",
    "            \n",
    "       # return v,v_, h_\n",
    "    \n",
    "    def free_energy(self,v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v,self.W,self.h_bias)\n",
    "        hidden_term = wx_b.exp().add(1).log().sum(1)\n",
    "        return (-hidden_term - vbias_term).mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructions error:  0.0161874972541 - free energy loss: 0.748246817753\n",
      "Reconstructions error:  0.0141719887997 - free energy loss: 1.70921167834\n",
      "Reconstructions error:  0.0137004646762 - free energy loss: 1.2363628519\n",
      "Reconstructions error:  0.0133602742491 - free energy loss: 1.10104685816\n",
      "Reconstructions error:  0.012947690898 - free energy loss: 0.936798621868\n",
      "Reconstructions error:  0.0128003399948 - free energy loss: 0.897213113719\n",
      "Reconstructions error:  0.0125741876405 - free energy loss: 0.911114922885\n",
      "Reconstructions error:  0.0124832134822 - free energy loss: 0.956266732051\n"
     ]
    }
   ],
   "source": [
    "rbm = RBM(k=1)\n",
    "train_op = optim.SGD(rbm.parameters(),0.1)\n",
    "\n",
    "\n",
    "batch_size_=32\n",
    "\n",
    "for epoch in range(8):\n",
    "    loss_ = []\n",
    "    reconstruction_error=0\n",
    "    s=0\n",
    "    for n in range(0, len(training_set)- batch_size_, batch_size_):\n",
    "        vk=training_set[n:n+batch_size_]\n",
    "        vk=Variable(vk)\n",
    "        \n",
    "        v0=training_set[n:n+batch_size_]\n",
    "        v0=Variable(v0)\n",
    "              \n",
    "        ph0,_=rbm.v_to_h(v0)\n",
    "        for k in range(1):\n",
    "            _,hk =rbm.v_to_h(vk)\n",
    "            _,vk=rbm.h_to_v(hk)\n",
    "            \n",
    "            vk[v0<0] = v0[v0<0]\n",
    "        phk,_ = rbm.v_to_h(vk)\n",
    "        \n",
    "        loss = rbm.free_energy(v0) - rbm.free_energy(vk)\n",
    "        loss_.append(loss.data[0])\n",
    "        train_op.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "    \n",
    "        reconstruction_error+=torch.mean(torch.abs(v0-vk))\n",
    "        s+=1\n",
    "    print ('Reconstructions error:  ' + str(reconstruction_error.data.numpy()[0]/s)+ \n",
    "          ' - free energy loss: ' + str(np.mean(loss_)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: Variable containing:\n",
      " 0.3045\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now predict on new observations \n",
    "\n",
    "test_loss=0                                       # need to measure error. loss function could use RMSE (done in autoencoders)\n",
    "s=0 \n",
    "vis1=[]\n",
    "for id_user in range(0, nb_users): #batch learning\n",
    "    v=test_set[id_user:id_user+ 1]  # training set inputs are used to activate neurons of our RBM\n",
    "    vt=test_set[id_user:id_user + 1] #target\n",
    "    \n",
    "    v=Variable(v)\n",
    "    vt=Variable(vt)\n",
    "    if len(vt[vt>=0])>0:\n",
    "        _,h =rbm.v_to_h(v)\n",
    "        _,v=rbm.h_to_v(h)\n",
    "        \n",
    "        test_loss += torch.mean(torch.abs(vt[vt>=0]-v[vt>=0]))   #update train loss\n",
    "        s+=1\n",
    "    \n",
    "    v=v.data.numpy()\n",
    "    vis1.append(v)\n",
    "print ('test loss: ' + str(test_loss/s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(test_set.numpy()[4]==-1)\n",
    "vis1=np.array(vis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis1=v.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis0=test_set.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 68,  79, 109, 166, 224, 230, 233, 240, 242, 258, 363, 368, 369,\n",
       "        375, 376, 387, 388, 392, 393, 396, 399, 401, 409, 410, 420, 423,\n",
       "        438, 440, 443, 452, 453, 456]),)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.where(vis0[4]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis1[4][0][240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   6,   12,   16,   17,   19,   20,   27,   31,   32,   33,   34,\n",
       "          35,   38,   40,   42,   57,   67,   71,   74,   79,   80,   83,\n",
       "          93,  102,  103,  104,  109,  114,  116,  118,  119,  127,  129,\n",
       "         130,  138,  141,  145,  146,  148,  149,  152,  156,  157,  162,\n",
       "         169,  174,  175,  176,  179,  185,  204,  216,  220,  222,  223,\n",
       "         228,  230,  234,  239,  241,  243,  246,  248,  253,  255,  258,\n",
       "         259,  262,  266,  277,  282,  285,  290,  293,  294,  295,  297,\n",
       "         303,  304,  307,  308,  311,  313,  319,  321,  322,  323,  324,\n",
       "         326,  330,  331,  334,  336,  337,  344,  346,  351,  354,  361,\n",
       "         362,  363,  364,  367,  368,  372,  373,  374,  375,  382,  383,\n",
       "         389,  390,  396,  397,  398,  399,  406,  408,  409,  411,  413,\n",
       "         414,  416,  418,  419,  422,  423,  436,  437,  438,  439,  441,\n",
       "         445,  446,  448,  449,  450,  451,  452,  453,  455,  456,  462,\n",
       "         464,  467,  468,  490,  492,  501,  504,  505,  509,  511,  522,\n",
       "         532,  535,  541,  544,  547,  550,  553,  555,  556,  560,  566,\n",
       "         568,  579,  581,  582,  585,  587,  589,  592,  593,  594,  598,\n",
       "         599,  601,  604,  606,  613,  619,  622,  626,  629,  632,  637,\n",
       "         638,  640,  642,  646,  649,  657,  663,  664,  665,  666,  667,\n",
       "         668,  671,  678,  679,  680,  681,  682,  685,  686,  687,  690,\n",
       "         694,  697,  707,  710,  713,  715,  719,  725,  729,  732,  740,\n",
       "         744,  753,  754,  757,  758,  760,  763,  764,  765,  766,  767,\n",
       "         770,  772,  773,  774,  775,  776,  779,  780,  781,  783,  787,\n",
       "         788,  790,  791,  792,  794,  796,  797,  798,  799,  800,  801,\n",
       "         803,  811,  813,  815,  816,  824,  826,  829,  831,  833,  835,\n",
       "         841,  842,  843,  849,  850,  851,  855,  856,  857,  858,  859,\n",
       "         860,  862,  867,  868,  875,  877,  883,  884,  889,  890,  893,\n",
       "         894,  896,  897,  898,  900,  903,  904,  906,  907,  909,  913,\n",
       "         915,  916,  917,  918,  919,  923,  924,  928,  930,  931,  932,\n",
       "         939,  940,  944,  947,  949,  951,  956,  959,  960,  961,  963,\n",
       "         967,  969,  971,  974,  975,  977,  978,  980,  981,  982,  983,\n",
       "         985,  986,  988,  991,  996,  997,  998,  999, 1000, 1004, 1005,\n",
       "        1009, 1013, 1020, 1022, 1025, 1027, 1028, 1030, 1032, 1035, 1039,\n",
       "        1041, 1043, 1044, 1047, 1048, 1051, 1052, 1054, 1055, 1056, 1057,\n",
       "        1059, 1061, 1067, 1068, 1069, 1070, 1071, 1073, 1074, 1075, 1077,\n",
       "        1078, 1079, 1082, 1084, 1086, 1088, 1089, 1091, 1093, 1094, 1096,\n",
       "        1098, 1099, 1103, 1104, 1105, 1107, 1108, 1109, 1113, 1115, 1116,\n",
       "        1120, 1121, 1122, 1124, 1127, 1128, 1134, 1138, 1139, 1143, 1144,\n",
       "        1145, 1148, 1149, 1150, 1154, 1155, 1157, 1159, 1160, 1161, 1162,\n",
       "        1163, 1165, 1166, 1171, 1172, 1174, 1178, 1179, 1181, 1182, 1183,\n",
       "        1184, 1185, 1188, 1195, 1197, 1198, 1199, 1200, 1201, 1203, 1204,\n",
       "        1205, 1206, 1208, 1214, 1215, 1216, 1219, 1220, 1222, 1223, 1224,\n",
       "        1226, 1227, 1228, 1229, 1230, 1231, 1232, 1234, 1235, 1236, 1238,\n",
       "        1241, 1244, 1245, 1246, 1249, 1250, 1251, 1254, 1255, 1256, 1258,\n",
       "        1259, 1262, 1265, 1269, 1271, 1272, 1273, 1274, 1275, 1276, 1277,\n",
       "        1279, 1280, 1281, 1283, 1284, 1286, 1289, 1290, 1291, 1292, 1294,\n",
       "        1295, 1296, 1297, 1298, 1299, 1300, 1302, 1303, 1304, 1305, 1306,\n",
       "        1307, 1308, 1309, 1310, 1313, 1315, 1317, 1318, 1319, 1320, 1322,\n",
       "        1323, 1324, 1325, 1326, 1328, 1329, 1330, 1331, 1332, 1333, 1335,\n",
       "        1336, 1337, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347,\n",
       "        1348, 1349, 1350, 1351, 1353, 1354, 1355, 1356, 1357, 1358, 1359,\n",
       "        1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1369, 1370, 1371,\n",
       "        1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1381, 1382, 1383,\n",
       "        1386, 1389, 1390, 1391, 1394, 1396, 1398, 1399, 1401, 1402, 1406,\n",
       "        1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417,\n",
       "        1418, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1428, 1430, 1431,\n",
       "        1432, 1433, 1435, 1436, 1437, 1439, 1440, 1441, 1442, 1446, 1447,\n",
       "        1449, 1450, 1451, 1453, 1454, 1456, 1457, 1460, 1461, 1462, 1463,\n",
       "        1464, 1465, 1466, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476,\n",
       "        1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1489,\n",
       "        1491, 1492, 1493, 1494, 1496, 1497, 1499, 1500, 1501, 1504, 1505,\n",
       "        1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1516, 1517, 1518,\n",
       "        1519, 1521, 1522, 1524, 1525, 1526, 1527, 1529, 1530, 1531, 1533,\n",
       "        1535, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547,\n",
       "        1548, 1549, 1550, 1551, 1553, 1554, 1555, 1557, 1558, 1559, 1560,\n",
       "        1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1572,\n",
       "        1573, 1575, 1576, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585,\n",
       "        1586, 1589, 1590, 1593, 1594, 1595, 1596, 1598, 1600, 1603, 1604,\n",
       "        1605, 1606, 1607, 1608, 1609, 1611, 1612, 1615, 1616, 1617, 1619,\n",
       "        1620, 1621, 1623, 1625, 1626, 1627, 1628, 1629, 1631, 1632, 1633,\n",
       "        1634, 1635, 1637, 1639, 1640, 1642, 1643, 1644, 1647, 1648, 1649,\n",
       "        1650, 1651, 1652, 1653, 1654, 1656, 1658, 1659, 1660, 1662, 1664,\n",
       "        1665, 1666, 1667, 1668, 1669, 1670, 1671, 1674, 1675, 1677, 1679,\n",
       "        1680, 1681]),)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vis1[4][0]==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
